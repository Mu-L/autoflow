# Deployment Requirements
In this section, we will cover the requirements for deploying the project.


## Software Requirements
* An [TiDB Serverless](https://pingcap.com/ai) account.
  * Currently only TiDB Serverless tier has the support for the Vector Search. You can use the free tier with 25GiB storage. We suggest to choose a nearby region to reduce the latency.
  * TiDB will probably introduce the Vector Search feature to the open source version next quarter, so stay tuned.
* Docker Compose, you can install it from [here](https://docs.docker.com/compose/install/).


## GenAI Services / API Keys

* An **OpenAI API key**, you can get it from [here](https://platform.openai.com/). It will be used for these purposes:
  * LLM(Large Language Model) for knowledge graph extraction and chat engine.
  * Embedding model for converting text into vectors.
  * Chat Engine for generating the answer for the question asked by the user.

* A **Jina AI API key**, you can get it from [here](https://jina.ai/reranker/), it is free for 1M tokens. It will be used for reranking the results retrieved from vector storage.
* An **LangFuse API key**, you can get it from [here](https://langfuse.com/), it is used for tracing the RAG application to debug and optimize the performance.


## Web Hosting

### Hardware - If you are using a Cloud TiDB and SaaS LLM
You can use any of the following web hosting services to deploy the project:
* Cloud server providers like [AWS](https://aws.amazon.com/), [Google Cloud](https://cloud.google.com/), [Azure](https://azure.microsoft.com/), etc.
* Or your own server.

We suggest the following configuration for the server:

| Name                 | Value            |
|----------------------|------------------|
| CPU                  | 4 vCPUs          |
| Memory               | 8 GB RAM         |
| Disk                 | 200 GB SSD       |
| Number of servers    | 1                |


### Hardware - If you are using a self-hosted TiDB and self-hosted LLM
If you use a self-hosted TiDB and self-hosted LLM, you need a powerful server to handle the load. We suggest the following configuration for the server:

| Name                 | Value            |
|----------------------|------------------|
| CPU                  | 16 vCPUs         |
| Memory               | 32 GB RAM        |
| Disk                 | 500 GB SSD       |
| GPU                  | 1 x NVIDIA A100  |
| Number of servers    | 1                |

GPU here is used for the LLM model, you can use any other GPU model that can be used for the LLM model which has capability more than gpt-3.5.